{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab8/Laborator_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmaHAc_KHG_5"
   },
   "source": [
    "# Rețele neurale pentru clasificarea imaginilor\n",
    "\n",
    "_Tudor Berariu, 2018_ \n",
    "_Alexandru Sorici, 2024_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKCKTWvsuJuF"
   },
   "source": [
    "În cadrul acestui laborator veți implementa o rețea neurală pentru clasificarea imaginilor.\n",
    "Rețeaua va fi compusă din straturi lineare și activări de tip ReLU și un strat softmax înainte de ieșiri. Funcția de cost folosită va fi negative log likelihood. Pentru optimizarea acesteia se va folosi SGD (stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgNIWhzoHOIz"
   },
   "source": [
    "## 1. Setul de date MNIST\n",
    "\n",
    "Setul de date MNIST este compus din imagini de 28x28 pixeli reprezentând una dintre cele zece cifre 0-9.\n",
    "\n",
    "Decomentați mai jos comanda `!pip install mnist` pentru a instala pachetul `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQiqJyO7E7Ek"
   },
   "outputs": [],
   "source": [
    "# !pip install mnist\n",
    "# import mnist\n",
    "\n",
    "import gzip\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\"\n",
    "filename = \"mnist.pkl.gz\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "if sys.version_info < (3,):\n",
    "    data = pickle.load(f)\n",
    "else:\n",
    "    data = pickle.load(f, encoding='bytes')\n",
    "f.close()\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElGfqnPzuJuO"
   },
   "source": [
    "### Exemple din setul de date MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeftJ_CpE7Eu"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avJh9wQquJuU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [9 3 2 2 8 0 9 4 8 1 3 3 3 3 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAABRCAYAAABykMTkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABEZUlEQVR4nO2dd3gc1dW435nZImm1WvXeu225IOMiYxs3jAGDARObEFpICC0QB5IQ+JIAgY+E8oMQY0qAGAKmmxLcwB1w75aL5KLeJatLu9rdmfv7Q7ZsucrWSrL9zfs889iamd17zt4p5557zrkSINDR0dHR0dHR6SXkvhZAR0dHR0dH5/8WuvGho6Ojo6Oj06voxoeOjo6Ojo5Or6IbHzo6Ojo6Ojq9im586Ojo6Ojo6PQquvGho6Ojo6Oj06voxoeOjo6Ojo5Or6IbHzo6Ojo6Ojq9im586Ojo6Ojo6PQquvGho6Ojo6Oj06v0mPFx//33k5+fj91uZ/369QwbNqynmtLR0dHR0dG5gOgR42PGjBm89NJLPPXUU2RmZrJjxw6+/fZbQkJCeqI5HR0dHR0dnQsIiR5YWG79+vVs2rSJBx98sL0RSaK4uJjZs2fz3HPPnfHzkZGRNDU1eVosHR0dHR0dnR7EarVSVlZ2xvMMnm7YaDQydOhQ/va3v3XsE0KwbNkysrKyTjjfZDJhNps7/o6IiCA3N9fTYuno6Ojo6Oj0AlFRUWc0QDxufAQHB2MwGKisrOy0v7KykvT09BPOf+yxx3jyySdP2H9z9K+wNzk8LZ6Ojo6Ojo5OD+Bt9eLjkn91aebC48bH2fK3v/2Nl156qeNvq9VKaWkp9iYHrU32PpRMR0dHR0dHpyfwuPFRU1OD2+0mLCys0/6wsDAqKipOON/pdOJ0Oj0tRp8he3khmUxgNiOZTQgvU/uBQ/WoDY2gqX0roE7fISsoNj8kmxVhUJDcKrQ5ES4XAGpt/f/N60NWUAJsSF5euEvPPFesc+EiGQzIPj5IVl+Et/mYAxIIgdTmwl1c0ncC9gGKn1/7+8LijTAa2n8HVUM0NqE1tyCcThAeD83sQDKaUKIjEE3NaE3NiLa2HmvrWDxufLhcLrZs2cLEiRP5+uuvgfaA04kTJ/Lqq696urnzDvvEgdRkGGlJb+OajF28ErkOgH7vPUDyu1Wo+w72sYQ6fYIkYYgM5+CvYnly5sdMtZSz2uHPsweupqw4CID+z1ahFpci3O4+FrZ3UZLjyfl1MCkDSpGvt6LpweYXLVJ6MiVXBpI8bT+fJS3o2K9IMs2ag/93KJO1g019KGEvI0mU3J1ByxA7Twz7htv9amjVnHzvsPLrr39O3CIX5i0HUOsbeqZ9WUEdOYDH35vLHUvvJukjFWXV1p5p6zh6ZNrlpZde4r333mPz5s1s3LiRWbNmYbFYmDt3bk801+dIBgMMTOOuTxYwwLQOm6xikWTMkoEjP/GyW1/gD5dfx4F3swh6a13fCtyDSEMHUDbORnO8iiW6iYTAWrwUF8/FfI12+JxXqsfz7beXkjw7D3dF5Wm/r6so/VPZ+1sbyya/jAxMXvNrzDt88K4WqGaoG6Ty7dUvoxxO7rpyza9J+ocKG7M90v7pMISHUT0lkbv+8F+m+X6CTTZhlIxM8m5i1IAPUAe0yzRrwDUU/y0TnxW70Vpaelyu8wVnlI3ZV79Hf2MNt1/xCNYVOT33sD2OilmjCJ5awk8it3CFJZdFzQOYZNmLl9R+tVaq3nxYO5LV84YRMXtjtw1DQ0w0ubOiWXrTiygSTPnXH4j/rAo194An1Dnvaezvj//kcubEf8nIbXdhei+AsgmC34z9jjv89qAh9bWIvYLi54djZCrT//Et03yfxyLJ+MhGVKFglBTGezezfsb/o+EmwX5XEPcv/Dnp/6pD3e3ZZAzJaKDoSi++b05HcspIovcGPj1ifHz66aeEhITw17/+lfDwcLZv386UKVOoqqrqieb6FMlgQI6P4cCjChO8y7DKJmRMFLnt7Hb6M8m7HqOkEKF4k+hTQ47XiUG3Fzqy1Yp7cBJFD2pMTNzHzdY8QgyNhChNWCUXbULhibKrmR3zLb6Smd+FriRgaisfeI8h+WHPGB9tEVbenvBvog1mDCi8M/JdCjKDadK8UdBINFURZzBhQMGNyjNDv2b2X8dTvS2LhMd6zhhU0pIpujaUe+/6hhut+7DJZiZkz6RqRxjmWgnNAPYoN2unvsRfohZy0y8iUE0ZWOZv6DGZAKRLBlA/wEprmIzbB8Zd1z7a2fHCEGyr81Arz+1ela1W1IGJuH0MmFbtOOPLWgkOoi7WTKKhlkiDmapLZPy2+EEPGh+SwYDjiktouLeJB1K/Yoz3AcIV8JN9uNVvL36yV8e5kQaV6NCVZN5TyD/t0wn/fB9qzaFzb1yW0MyCBKMvAG3pduzx/ph6K8FPklCCAqmalkr9BAcjE/IB2F0dTuvOAOK/ae5Rg9x/XQlVvjH8/pdTsf7TD6+tB6hPSaO0LaDH2jyCbLHgvjSV2nQvHEFHjRwhgdsikJObiQhopHxdJEmveW5gdDIkmx91qSZmWnOwyd4d+7XDgyMZGZvshU2GMKWBP07+L3+XryXlQQ/LoSioiXau89vO5kFxlG1OJHCNoVe8rz0WcDpnzhzmzJnTU19/ciSpfU7dasUVG0zZaB/sEe0jGFuORNj6BsTuAwiX52JMlJgoCn4SwTOZ87DKJja0GXni4DQKc8NBlfj1xO94MGA/ALIkuNgMe8lgQCTHUnCtN29c+i/ClWYqVF8W1Q/mh4okqsv8MdQa8DsIlyYNQCgCOcrO8LhCbhy3gR9uHYlt3oZuz2mqXjLjvFxoh+vmZZlVssxHHx4yUscxGZlJPiXEp37Gl2FDWZafRci/N3n8hlNSkyieFsqAaTn80pZHoRtu23cDbZ+HkbizGaWuBYwGnGG+TAi5jxUjXufxfkt4YsjN2NaG4y4/MUaqW8gKrddfSnmWjBRlJz6slAGWOsyKm4m2PUz3bWTsLyIoiU4mcmUIYtvus25Cighl/61mfCOaiSmOR8srOv39FmDDHibhL2vIyAQMrUb7zNINJU8hl9mMHBNJzWXhNCVIeF1Sy9uD/sMgk4JLGKhUnSy1B/Bh+Uiq7RbKygKR7AoAQhYoNidKVPv3dAfhaMNSpHT87e3jRPXyvL4nwxARTktmDKWXGwgdVMkgnyb8jA5cmkJmWAmHxtaxIzyW9J1eaI6eyTRUyysIXWVgj6kf4Rv3oLXYcfkKAgytVKiwqTYO8GDcjyRhiIzg0LhYGuNlnP3t9I/OJ9qnvuMUGYG/sZXx1j3EGxqYHTiORYZhJD5R59F3xbGI5hZs+S5mFV/DU1ELiDaYkU9R89MsGbnRdz9LBufRNHoI8o/bPSeIJOHl5SJSUbk3ahUPRycR7OOD2tjouTZOQZ9nu3gK2WJBHZJCbbwX9mCZ5gSNWVcs5Grf9gfoY8XT2B2YRoyUAufwUD0Vrgh/IicVM9Wnmh1OiT/k3ETbglDi97XRkGgie0QUBOxnY5vEyooUvGq1M3/phYSi0BLvy7DROVhlB38uuo69leGw20rwTpX+2ypw5xcCEHz4I9roIaydmcY/przPwusHYPtQBnHugZZKQAAtoSdeyvJhS087SR29ANmbYWZIDl7DymkpSP/xrLUvW61Ujgsl7up83oxbSIlbZVbeDGrnxRD63/bR8xGNlRyFcHMm1cMMTLWU88yARloHx2DykPEhe3khhwTjSAnj0C0tbBn5FjbZmxq1hUK3kU32BH635idw2WcsGfAJD9gmstE8kPj6uI6+6yqa1YuhA/OYl7iYy8Y8RGhtA2p19SnPd4XbaE50Y5UNuISKl8ENkrG7KndCMpqQ0hIoHxtI6A1FPBG3jMneLbgE7HW5WNGSzsKKgRwoDCNgoxFzgyDtQAuSS20P/iurQU0Ip2qYhHC6uiWLaG4heOfR70gOqqHc5of3aT7jCWSrlcaRcZRcKZgxYh17G8PZvj0Rc42CpII91sWkIXuYO+EdnreMgx4yPoTbjTuvgJB/FaNqKobwMEhsIdp0iPmNmezfGEeih4wPyWBACQmm/No4vK+v5IHYTVxhySHO0Dmm5MhLX0MDzDwT9iMjpx/kvWf69ZjxodbX47OxgM3L+lF963KssoMcl4Ud9jgKHEGEmxuYFbCv43yb7MWIgAI+ykwh7EcPCSFJSN5exAbUoSAx3rsZR4SKFBQAuvFxBiQJyWBE9vPF1T8O5akq/pnwFSkGFw6hUasp/LnkWlrdJm6LWMeBnxbxr5CJpO71nGUvFJkAcys7nQoP7P4pfq/6YTa6KbrKyG+mLOJX/gdoEyo/3/grQj/zJmhtAR4dX0sSksmE7GsBSUK0OUFVEU5nrwUuqmaJCK8G7tl5GxF/VInbv7ej7ZNJYNyZR2R4P55Nupqs6AK6G9vuGJZE7QQHGuLwA0RGQ6NZc9EkNJo0BRmBCxkjGv6yhlU2YJaMuIRACM+7o9xDkom4pYDZCZ/RoMFrNWMRs2wE7lyPeryXR1MxLdlEtWqhn9HJHanr+de4ySQu6Z4MspcXGI2I5FiKJ9gY99NNLI/cRIMGRe5mPmq4hA8ODEPd7E/qs+t49IVbSL/pZebG/sCjM5pYqI0i6rmzMz6UqgZ2L02FexYT8NMS1J3hcKj2lFk8NQO9uX/0txglhUq1DfebYUiFOd1T/HiZoiM4MCOAP0z/gjv92l9szcLJ5jZfntg/jaYl4UTN209qzRZkb29kPystQ2OpHmREViH61ULYmE3oRuhuLpLW0oLXmhxaNSc+somvUr4leeB9+Hdby9MjUmMpmaoybch2Pl81kvQXi0gpPTy1J0kwLIMVNw/kwRuXg6yc/ss8gaaCrGAfFMODA5fiJbt4Z+1Y+r10sNu/8RFkmx91l8fz2WMvHONZ8Dr8jOhMq2g3Moy0x1zc4FvFf0yDoNVDwhyPEIimJoKyNYySytLWWJ7aOhW/ZT6EbK7n+6H+zHp6X6ePmGUXmgftcslghLBg5iS+g4/c/vt4RzTT0i8U81kOOs6FC9r4MERGUHVlHI88+jGTfL7BKpuY2xDPz/eOQ1ljI3pxNWpuHmgqb116HaX/o/HT8WuY/+gYYp9a6xEZjIda2LEylcfWJBJa0ULeT4zcdu1KHgvaA0CbENyedy3Jf25Gy9+D29Ou/fRkCq8P4T+/+gdDTAYu2zGDmt0hRK9y471yN1prT9097Yi2NgI2VrD6tREYBKg56884haI2NmLbXE5JdDSv/O4DbpJHI7rhEGqOMDIi8eBhT4eMjISMwtTdM1HfD8U2b32n8/P/nsXdU7/j4YD9GCUJSfJ8GlvBfYK3Yr4l0mDmodKxFN0dj7Zj72k/89vsGfxr0AdowjNLLhX+PhPfkdXck7icX9iOelGGLPgN/V6pQ927n0jar1MkDxlgbjemBlCFYGm/b0i6+V5SRH/EphPjCCSjCbcPxBhrAShw++K33PPBpgU3R3HllE0dhkebcDN09f2EfWXCf10JlpK1aGYz8sA0cn/hjxzcxisj3meSdxNftwTzx+ibSXnIgzE4Lhf3Fk/itZjv8D0mvqQnkfYV0f8JP/aLUJJL13caFBgS4jhwnS8PT1nAL3bdTuChXsjIkxXEiAyuemklM/328Eljfwz1yjnHGp0MLSEScUc10YbTT5XVaQ5+lXcjQeYW7glbyZFkG+clSRg35PTYM1RzOLB9u5efx8wiakU9SfsPIEWEUj45goG37eqRNo9FqCpSQzNv1Y7i8ZCNmCWZ7SP/w3jLTzAv6vHmL1zjQ85Ip/jKQNJvyOUqSxn/Uz6BdW9nErq+nriyKoSjCLWl9eiIa3sO/u8PZd41I0geW4xkNnskn1nbl0fSP2ogLJjU9/P4W+AaEozt7jsARZKYEbaZ/7l/JvELAjBvz0c9VNvtdgHq7sjCPb2Wjwa9RJqxfbRyV8JaUtMqeHHYlRQMHELcp2W48wo80t6pcBcUE/JxNQiB1sXYDWdMEM7LmlA9tLSQhtTh+ZBRyNo+E+O7gQQs3HniOEeAJmQ0hOc9H5KE64qhPDB4Gf1NTfy2bCJrvriE6N0bPdfGaVACAjj4SDqzb36bJOOP2GSJ2bXDSfr0egJ3SISuLqdfwwG0hqNuVdnLC/vEgXxz00ukGr14pyGcL7/NIvn1XWc9CnVXVBL9iWD0xNtYmfnuac/VLu1Hy0AHV1nKaNI0Htx5O5GuorNX+jQYYqLxuayGX4esolwVvFGbxdcfjCHtg4No9Q2429qQDAaklASq/ldj+aAXMUoQorSPBIeYyxg9bC+eDD0UqsaPW/vRFLUI3x5bV7wzWlMTWksrx1v5ksFA7gPhTBu3gf+WDyboSROiB2vNGBLjqRkdQe0UOy8M+5zx3tX4SF781G8PRZMDWVI7isjnPTMwlO0uKsoCYFD733Wag18XXsfmvDiES8a7wET0ilbQBMbyOjb+JInoW+sYHLwdgJLxZpJy/Hp0AKc2NRG9pIa6wYG0jh9M67BW3hrxBhmmJuCoYbq41crsreNJn1/iOc+5pqLV1fPxjmHMmrgOs4enO8/EBWl8KGGhFF4byMCpOVwbvINLv7+P8M/MhG8uRq2qRjuJUSHcbvw2leKTFktmZjE7+/VHbN/TbVmE241a14AUG8FdQT+SZlSQj/lZZWRGexfz56u+4OPBw9i7LxnbHgPh65pOOhrsKk0zR2K/voGn+y0kzajgEiqrHT68Pvt6nDZQRtQxdvpWvhvYj7RH7D0auY2mnnVqqKQJ3C4FBYmGn1yK/3+zzzm91PuQSnZFBHJ8u+cDoOZAECnFrad9cMhIHvd8SCYTfv9TzLW+u/ikcQBLl19CytfVqF3weN2WvJFko4PV59i2EhBA89gUHpn+NZN9XFydO4P8VfEE71RJzW9Erq4/aREv2d9Gyc9cJBvNKJLMN1WDseZxbkFnQqA1NtGQl0DbJad3ZwmTjGzU8JIMtGptNBfYQPXwi89owGRwY0RwwOXHvG3D6fdVOe7KqvZigJKMNCCFgzf78/aA14k1+ABQ5G7lr+VX8f3aAYRuBj/Wn6Ghs0Ny9kHk+XFGhezlxYG/XsLksdtYWZqC14cB2PZme36l0WMQZiP2YIk7MtYz2FTBSns4D6+ZSUZCKTPDN6HNlPg6dCSpc+vQcg92b+q4uJyk9y2kS/dhqDHidwD8itwkNziRVBdKQz2iuAw0DYICcfoJ+nmVdcSAqCZA6ZkpKMlsRo6LJve+EMwxzVwWk02c9yEu8SlguNmBUTpqeMxvDuaxVTeR8JmGWurZIHTR1kbIchMt4wU2uXfjES9I48N+SRzy8Hou8z/I83snE/GJCZ+FW047paH420AIrEUa8/cOIXCINwHbPSeT3ObimZJrsBqOGj7eiotLfAu50TePm3yLuDwpjx8i4/k0fRg5cXGkHYo/Z69E1aUSdydv4nKvKlo1+LY1iv9ZOJO0+QeQLD4Uikjqr63gD5nfMj92IvSk8XGWKKlJlF7qw+jEbGRk2mwSUjducktePVU7gmHk0X2GUDsuPxPH2/KGmGhcoS4SzO3uXY/dbpKEEhxMzTXJLEh8ER/JwGclmQTvFKg5Z67hIJnNjPHZgk32wiUU5LN4B7ddM4zWEAOOIAnHsGZ+ZSsjedWd2FZ7k7C8HPVAPhon11W2WHCmRfKnzIUYpfY+2LEnjpTsboz2NA1Dc/vL1RjdgiPYi5M5vp1WI97ezUB7LIWhWUJ4uJKjaGqhZlMyLwRNIsTURNCPJtQD7emlSmQ4aqAv5aNsZI7NYYTZRZ3mZFbxNWwpjUHZYiXp+xaU7DzPXScnI8qONHQAYovnAuFPiyShhIZQ8rNkRozew+aqGNQVQfgvy0Xt4foyUn0TAfsC+Pf3l7M4uT+V2WHErnBTmJ7Iq5MtPJc2n6gp9bxdN4X416pQ6+rOuS21sRHDml3EmwdhrmpE2nuwU6xfxy02fCDlI60ED6sg06uEIx4H3xIJ0UOBt7LVl8ZBwTx85UKGeeeRYRQd9x8cfRY2aA6e3nU1kctkzGt3o3k4AFZoAr8CB64+SMO8II2PQwOMjIwsZHVtCgH/8sW8aOMprXXJYECJjqRlQBhOXwWjXRA9z0hNhoTHMsuFBlWHOPh+KppytBM1MyxMG8yeYZsJNTVxqU8eV/gUcGNyCauj/fnzgZ8T8mbxOZXUVgPbX6Aqgh8dYfx5yzSSH17fcUPFLPFhk386D8xYzr9TLNg2SaeMxTgSFS4s3kiaQDQ0emxq6Hhkq5WKCaGEX1fE05GLeaN+EP4HnO0lhM8Rdc8+wjfaqPplK4GKGRmFhwcv45WR04htHITkUpEL20cMlZNjyErPYai5FA1vVCFotpsJ6q5ePj44M2IYfN9OgmRvVjmMlG8PJ/FA85nTiCUJKS0BL+l7NGTyW4Mx13btYaD4+VF2q5Mb0jeRYK7mCksuL9UOIvmOPQiX84zTJlJ0BMUTvbjTr90Ye7cxlIjVMtK6HV1q/0zcnr6RT5MnEuHjc4IXyh6sEGpt9kg7p0Ktribp40CW2AYzaHAB1hIXSv9UcLqoHR5GfaqMd+Yh7gr7gZV2XxbUDSHn3/2I296EnJ+Deqi2Zw0PYFB0KUWZyQRt6eGGoKMcQcPlifzp3nn8p2wUfBZM9JKDuLtTv6SLuMsr8FpQQeoSA63XZJK8sD3FPWprKDV1SfzrV+N4J24p5dNt7FqQjtTc0q2ME+FyYvp2MwJO+o5QgoPYP92XayZt5O6gH4gzmDoCUkM3NyOae8YYk6y+1Ccp/MpWAKdIsQWo1UDd7Ydfbl2PTf8oDjfFbj8ilRaMkoJRUZFPcr96mgvS+DA1CjaUx9JUYSVt6fajF9WxQXOSjGQ0oESEsefxMB4bsxCAvy+/lpSHthK73uaxqGqEQK05RPCbJxarCgd2AXLGUN65dRLTJq/nqdANjPdq5OlH5jJn3vBzcm8HbDKyeMAg8i2hvLlqAul/zeukj1zTgFetP+FKK1UjBP6fGhFuV/vvIksgySC3/15KRBh5d0QjDWzE2WbEf2UEQW/3QOEtWcE+Nh2/G8t5K+Vjniq/krIbbRhLt3T7AW9scvFi9Vj+Hr4JgLttxdx9z6twD6xyGPn9i78C4N1HX2aA0QS0u9dbBYhsP4TaPQmk+GhKxptZHP09GnD3wl+S/p/aLlUklEwmcn5tJVhxUanCiu396beoukvXZ8vYdO4csJrHg3PZ3tbGzdl3ETh1H3CGB7asICkK1ZeFkPPL1wFo1Zx8MmMC1p2em2J4KHAHbw8dTci2VJRNnQNunX4SQV4tp6xv4Cmk+iYMrYFcEbKHlX9q/1V3lUdwTdJGbgtcR5ji4oPGwbzzxWTi/rKOINYh6H5mS1fZWxlOdE7vrOAt+/jgGpzILU8uxKEZaXglluDvc3H30GDjVAi3G++vjw4a1coqgubbKalOR33zO/4UupbLrsoirimy52LWZIXKG1OZP/Nl+hmNaMe9DpXcYtReWufkVCQYvLh3+mLm1l5NePapB5DnjKbC9lyeL7iKl5I+I9UoE2VpoHJoGvIP2zzb1nFckMZH7RCN2xJ2si0ghopbhhK8sD0lqW5yCr7FbbREmqkYK7hu5BbuCfqYRKMRGZlK1c6LwXbQ1B4b2Z8KbVcOCY8rrN84nBG39SN7xIdc4+Pg6Y9DCfxNMOr+vLP6vpA311P+tkI5NlK0zajHeE8UfxstgyNpTnMSb/Bh142zuXrxA7isCo3xMs2JbgakF/Nx8pfIh9NSjZKCjEyNaufF9LHsetvTvwAUPjmcW69fye3+m3kofzqOWcGIUs+4mo2F1Sz9cCTPPryBI6m2R15qWWY73//Py7iEhq9sxn34tbLMbuXBhfeT8vSGbgfZNafYSL88Dw2NzW0Kqe+3oh0oOOPnJKMJKT2RDya9SYhiZuaBqYSsU1D37u9SuxVZCv29SwF4vzYLy+u2rn3uNyO4dOZOvo75J2BEFRqTfvcQtnzPuf6NSHhLJj6+/A2KRwURbqgnw9SGEQUXKkZ+QJHa43R6MrnTXVpG/Deh/D/r1eRe/1r7zsT2f2SMvNWQyJuLJ5Py9x097uU4GUOjitk9uD+hP/RsO0pwELVXphB7335Geh/k93ffh+/abNQeHuF2Fa25GZ/cGoZv/DlbRryLamovZdBTSLJEcwx4SSqcMEELOS8nkvA+Hk0SOBfu8NvD51MyaagZge0Dz8YeQbt3yO4+qv9gv2L+k9WfyB6+Hi9I48O2V6FkaACvxH/Onj8HU/xou9M8xrSGJtUbp1BoUC2sPJTKHRV3UN/owxOZCxhoLu1bwTUV64ocNCWd+6NHMjtyLX9P+4K/+/707L9LiJMGYykBATiGJVE0Q+XNy94HwCgpXP/iUmKMtfgrLfjLdkIUJ96SD4okowqNbKeL9fZ45pdnUroyhhg8E3EO7aOtff87iDnX/pv9beFMfvf3JM6rhgLPrWehVlQS86lEevIDbL/2FXwkU0eRsXbDyoBZEshIfNQUwZsFY6n7MZx+7xbi7qbhoSQnUJWp8HrcV7Rqgntf/w0x+3d3adSkhIWw7xZ/MkxtVKsquSuSSNhU0+VRd/B2jf3TwsDSzLSArSy7Ow3XkFGEbXZSM8iEb4mGZpBoij1mOnBwE78f+AXX+h7ELFkocjczbuVDpC/dh9rsgWkQo5HYrBKMUntGUX+jSoqhAqMkYz7cL8bjzA0vSSY2qwTJZPL4qpqGxHgKx/hw9fAtHdeEIrW/1NJ/vI3gT31IW1eIu49ewjurIgjO6fkRtmNIPJXj3Tweto5fvDSLiA3ZaHZ7j7fbZYRAam7FkRcGI3qhObebpP9UUvEzCwmGE++4r8bNYZp4gCQ1HmWVZ40PraaW6BV+ZGoPknLtfgbbSvFR2q8Bq+zgCksusYb20nO+spnnUj/njqvvwvaBR8U4Ko+QUI+J++iB0kcncEEaH2HrG1kbNIjbskK5OnI3ieYqCp3B+KutfFGdyZaCWIx5XtgOgFeDSoBNYW+/SDY0JaLk+Pap7Gp9Az5VTkpa/dEQxBgaQTn3npaMJuT4aIqvD6clWkNY3ERF1/I/cWsZ5dWEdtii/7ltLz6SiTrNwSp7JK9WDsKlKazJSUapNWCulTHXCXyqNOIO1ntsBGiIjqLw1jhuvHwd/yyeyP6NccR/Z/f4QlrC7UYtryB5XjBN16j4KCdWNj2Shvty7iT4NpC4H2txl3TfILUnB+NOcpBgUGgWLmIW1qA2nvklrqQkUnJNOHdctQIfycSQRfeRvMKOKC7vctv+m8r59+dXsvLyAm6PXMdzGV/wY3wqmybEMdm/jGJ7AAZJI9DUyg+liTTn2zDs88U6xE6o0l7Wu14zYNvghVpb1223rmQwIIUFc3Pkug7Pk1FSOoqI/Tr/ei7xL6aizY9fhHzfUVPBLBm4M3otHyRORsrJ85gBYggPo+DmSFKuPMgvg7/n+BGuxbsNIVtOmgXUY8gSfon1eEkyS1rNOPb6470737PFB0+CZpIx+LgYaKqieaSdlqJ++K4vQDtUe16spKz422gdHMPIUTk9PhV3BK2ghIdevp/mWIFQBMIsSEgr56O0jxhg9OYXQ9cwf/MEInYFdW9Nn8MYIsJpvjSW2n4G3F4Qus1NRVESJd7JHCnv4/STeHN8LRsvndfxuWxHDKKi52rCVG8KY1NcPHGGfNLM5bT263lj+II0PqRdB4gjmUNFYbybFobLT8XYqOC2algPKMTkuLDklqMWlyFcTpw/G4nN0Mr8/UMI3tHNUa6/jZbRabi9JSwlDgy7888uZkNWUI0yvoezYv5TPwLJ4T7n9DbZz5fmAcEMn76TW0LWESS3kmDU8JFMgJEGzcGClgSyW6KpbPOjqCmA4sJgLAeNSCokr2/FVFaDaGpBa2xEtLV5xPCQjCbkhBgqxoUy7PpsKtuslH0ZT9K6JuT9RT0yny7cbuQft+MSdJp2OWJ0HMHH7KTRBzQvz+S1N0UbSI4owygpqJoTdc++M35GtlppGBKCz5WV/D4om0+bw0n6xI1h676zSjl2FxSR8Jk3FZVxPJ4VzhXpexnkW0Ksrx/1Lh8a2rxpdpnY7QinOc+Gf66ET7VG4fXBQAMNmp3VLemEbu1CYGwXkH18aE0NZoLPAYxS+8hthxNWNvfnm9KBNC8KJzc0AcUhsXt0BI8lL2KSdxMAChrC4LmXjiEinLIbEhlwdS5PRC8g3WimUXPwbPVl7KyL4pG4bxkRXsTK9GD8rFa0piaPtX06JEXhjuT1+MhGPqsZht9BPL+Oz0nwqnagHPDl3QEjmN5/G59Nz8SSkkzo1ja89pT0bDp+V4gMo/wyI+9FL0DDjNQLc2DC5SRs9lqiEuMRBgXhY6ZyZBQP334NL8Us5Ke2zbwzcCwh22OQuml8GGKiqbg6hvoxDiak7KZNU9hVNYCQf23sSDqQfXzQBiWTN7yzofF9XSrW/J4zyKJXOVk5MZ0JPgdIN1UzPCWfRj+/Hl3j5YI0PjSHAzbvInAzBJ7inCN2vGyx0HhDMxMse3mzYDJ+W8+9SItkNOEYnkLin/bS37eMN5ZeQdq/ws6qDr5i86MhzMhgvxI0NOYtG0NaffE5v/Alqy9NkQauDMwmzdiAAjRpUCkc1Kpe7GxL4bkl1xG0Q8Ja7MS3opl+lXmdrHiPj3kkCTk+mqIbwhh701YeCV3GDf/8A5FvbkG0tfV4IJ8itddXkTvciEcXlAP4fuDnvBMXzd+Sp9KvMLTbVRUdwRIDbOUnLdt8MiSDAa1fPBVZEnPT5lPmbuNPK6bTb9u5pTqqu3MJ2Q3hX4SyN2sgqzOHEL7eDQIs2WX4luRxrL+v7o4srHJ7gOMup5nX9owldv3Os273pLpZfakZaCTycFXJGtXOs0XT2f9tErEL6/Dd3b4svezjQ8P+QTx523VMumQebcLNxxXDkfYVnbROz1nLYTZTOz6e3zz0OTf4Fh72+tlZ0JLAqtkjCV1axG9fmcHd6Wv4MbMONSPRYxk+pxdMQrJYmBVQABjZUBZHYHUvhbZu20uCPZkvmi/Ha2wNfxm+gGFjC7lh3b2Ezo/HuqChxxaU6wqtcTYSxxQSoXhTrtoxtoDk6h2PzLFBrWE5XhxoHsKmJ9cz2quOYRkH2X9JGqFrutGAJFEzPoar7vuRPwZvwigp5LpUpmWmESpLCK39/UJiLKXjfHl35KsdH9XQyGsIwlLRg8Xflm9h2/0DKAv3IcPUxhWBe/gsdRJs7rlKqxek8dFVJLOZmhmDeHfoPyl1++NTIaFVn5v1KpnN0D+Zm19ZxC3WPP5ROwTvChlRdHbuWnf/OKpGCR4OzKFOc+K/V0K0nPtcs7ugiPB/V/H2tuv56+/dyLKGEBKOXBsBOeBT7SZ54dEgpd54zBnCw9jzWCCvjHmXTHMVrx0aRcA+t+cLSJ0CVRxZJOrYBaNkXELFKCloaIzxPkDrmO94/Z9jiZ/puZLOXdFQTksi73fw3+Ev4yWp3HvgZtJ/uxO1mw9+tbIK76+qiPvq6L6TPbrffuplBpnaR1YfHcoi/J2ec+fevv+nNL8dRcznmzvVKNBaW7FUtJHf2J51ZJYMzIpeyvPKmG63KZnNaJnpTHn0e27yLcJbMlOltvKX8ivZ+3wGgfPX4QZi/h7IZ09n8mLG5zzw4M9I2tTzS4krNj/qJyYBS3u0nZMh3G7ErhwidwHPw6dpY3nqdwH8afQ3fBgynBb3ILy/6p1KvKdDQ+OJsquI/rwId3F3V346h/YdDmwfrKfg8RBGmA8xzL+AHWGp3fpOJTgY39tLmeG/CbNkZJ/LyR/zp9Pvz4Wobnf7wqiDkzk4w5t5181m6DGFcUrcbVQdDCJ9W1WvPL99JBMDvYp5daQfoVt6IMPmMBev8SFJKCHBDLonmyC5jQf+dgcxi/POOahMCQxg/0w/brUWYJSMzF0+juSVTV12kUuXZlCf5kv9tBa+GPZPWoXGc1VjCP10d7ddW5rDgbRhFzG3HfMSUdX29FFxsjVde47627JIuDeXLyPn8HXjJTzy5R2kvF2Fd97Wnn+w+/lR9G4MIcp6DMdMs5S427ht7+0I4MsB7xMke5NslEn038+NWbu4av49xP7sYLdHfbVqG89XjQdOvfKpPLgfOb/14enBX2GVVV6sGo/21xBw9OxDVvG30TC5Hx+88CIJxnY/yPxmPxZuHkzqd54rMKE1NhGyw8XmNgUFwaHPo4n4sRj3SWo1SC4Nza10GIWyB/zsktGEuCSNG99Zxs9tBRgw8X5TOH9degMp8+z4bt7Sfj9IEjVDfBnsn0+8sZ5+URW4zOYev0Ylq5XKkWc+rzdQ9x0kfZYPf3vyBiaP28ae+2WUdd33BJ4LyoA0ysYY+DT+Sxa0hJD3TD98qj3jjTtXrvXdi1U+/bowXUH28sLwucI7CR8SrLRPRdZrZkobbERZHTiGDaf8VgePD1nMVEs+VtnEkYGTS6isak3GUqigHizotiynw/crP14Ivor5yYvxklTcPj3a3PltfBiiIhFO12mX5D7lZ6OjKLg1ln9GPM8VK39D2sY61OqabsmjGY5WoTO0Sigtpy/iJBlNyKkJ7L89kGGjc5hoKyLTuwCHMPC70rHseTED3+bN3ZLpqHBnX+Lck0hmM+6sASTcm8vjUYtINMDB1mAsxRKSW6XoseFE/ujAeKgVqbSqR1LXtLY2vBb60TpMxSipHdMs1ZqZil2hJH/czKiHH+DV4R8x0bsVGRkvScLH3L1l0o9gk03cGrSWPzPshGOKv43iXw5g8k/X86j/dnzkNm7PuRXX2+H4bd3d8ymekWHYb63rMDzebQzl2S+n0+/9Q53StLuL1tKK9+Y8HnruAQDCl1egVpz8ZWbYlY93TgYrsgK50qeBGKUZ+4gUvNbmnnP8hRwXRfFYX6b55mLAh90uJ08vvpHEr53IOw+gHTYuWqYPx/cn5dwV+j1NmpGqFl8C1PpzavNsEL7eJA88amg6DvrhU9LYqwOEo8IItJYWkj5rZrFlEBMy91BuDcGjC9lweDpBlpCkwxVsNXFC4bCqkYEkjCjGKGk8tvV6ktcd6LMaG7LVyoE/ZWCT13km8FWWuTViHTbZ1LErw9TGa4Pm8dWnQwk27mK0JZc0ox3bcQsNXr1nBvYPIohdXYy7hzwQRzA3qtTYLbQJNzGKxrU//ZENW4ZhWrO7R6bjzmvjA6MBSTuHx7IkoQbb0DKbiDV4Y9toRqos6njwnBNCILukjkDGsEsryHNHYC3MAgHGVoH/ioO40qJoifLCZZFwWSQaL3Vw1yUrudN/M3luX+ZWjWFVdjoBWw2Er9zn0Qd/X6EEBWIflkT1r1p5KWrR4fVtZK4OzGbfVaHsGRTC4NSD7PJPQDMa8S4PxLtGoCkgCQhbUYkorWhP++vGDSacTkK/r+Sb5iSm+R7suJHDlTYyhuWTbUggKz6HcKURMBwTE9JN/R1Q67JglBRSDG3U3JOF/34nsqphDzHRGizj8pO45IZdPB22HqOkMGXvdBq/jCB85QHUHg50dE8cyv7bBf/o93HHvs8rhhKULboUHHtWHK6hE76wfYE4tbL6lBUq1cZG/PervJh3JZcP+JAQxUBTrBHvrV5wjr9Ja2owUVMKCVa8aRNublp3DzFLVUw78tGcTgwR4bRcEkPpBMFz8SsJV1r5e8WVOJaEojnzz1ntrqL5mHggtr3g4cJWLwL2glzUO+70UyHvzsO3YDCHBljA6MFXgqxgiIog7xextAVoIAskt4S5RiZifRvGH9vjCbSh6Rwa4eahqPVUqxaMO3w9knl1TkgSss2PKyZsw0syoKGxrSEW7+puPCs0jedyr2TckLkEHB68+kgmhpsFw8OOHXweNTwaNAcv1FzGoe8i2w2PwuJzb7+L+ObUsm9XKP+JTeDntgLuDVrLRzeNpP/BULSi0nOqxH06zmvjQ9gd4Dz7Uani709jki9Tkzby35YAgnfbEd30Cog2J9YCWG33YbRXC6+nfcjXEUPYWBePW8iUN1mptCVTl6ERkFBLjF8DYd6N/L/IlcjILLOH8tzBKdSvDid1eRNszO7TB46nMMRE0zgsipLJgvyR83AJI0VuO04hE6g0c3PcZsoj/NlQHY8a6CIyoo6KQButTUYUmxO1xYj3oRB8/bxR9hd3bwpKCNT9ebySO55hQwoIOFzBNULxZn7yYrTk9jof2jGGhwzdXljOt1RlXVE8u8PcpBoN9LtjL2t2piK5DfhENjMssohQcxN/Dd3EaruFza2J1CyIJvrLg7jPwat3tlQMN5N3xWsn7Hd5Syg9EdEuRJdTmP32NlCwJoLcVAOXmGQakiHM4g3n+LM0RxqYm/QJMj6Uq06Cv/LGklMBvhZIjqZ8qC8Nlzm4sf8OQpUm3qkdxapVg0j55ECPDwSUgADqk61cZ2mf+v3tppkkbms8J8+uJ9HsDgyt0OD0wtuD3yubjLQlh3L3TUuY5LsHL0nFJWTW2JP4W+w1xHm1LzdbMkFh2tDNxJtqmFs1hpAdrj4zPJTgYBqHRfNAyOcYMFPkbmX9/kTic869xLvmdCF/EcSilASmWvJP8G4cS6VqZ60jigWHBrNh2QASljecc5zi2aLmHiDyx0D+GT+eX2S9R5hi5tHLFzJv6VSszS0eSTU+lvPa+DinuUdJwjkkgdLJGg8G/8DED39P0vqt3Y6gV+vqCPn3Fu4ddRufjX2DRINgVmA2StCuo665zHaLVUFCliQ0IWjSNPa4bMxadDtJn7dh27GrR9OXehPJaKJkeiyZN2fzaeRi2oSZMncbfy65lnqnN3v3RRG6xoDtoB1LXgX9KUUL8sfW1oDmb+HQQF/8DzooG+VNXaqN2DYVdnX/t/Ge78+KlHRi/PZglgwokoSM0hF4emwwqksIRDcr6vgtzAYG8tuAGXyS/iFz45ZD3HI0NFxCxSHaX2plbsE9q+8jarFC9PrCXkltlIwmxHHlQ9uEi38nfc7IMQ8RtDsePJTpci5ou3KI8x7I3yddzQeJCxkxZi81n0ZBwTl+oQTGw925yRGDd42LpoGhNCQYaMp0sPzyF7DKEg4heKjgBgo+TCb163zcvRDn4BySQPW0dvd1ndpKyu9rPFJnprsogf44/cBmcuBUPfhKUBRaQ02srUtkY0M8ABnWMib57mb1VS/zzZh+KGhM883FR1Z4onIM2+YNJGyR5wocnpW4Nj+aRyUQ8ts8ko0GNASfNl5C8EoTxu+6sdyEphI4dx1PjZyG17jPucpSdrgUwlHahAuHUHmrbgQffjuWlPdqid+97pRr0vQUvityaIrMoGSYnRDFwM9tBbw0w453ZfdTjY9H4ix0++Mf/8iNN95Ieno6drudtWvX8uijj7Jv31HX7cqVKxk3blynz73xxhvcd999XWrDarXS2NjINNvttDadffU9Q1wMuQ9G8dr17/Btw0D23hzfvoqlBy3p8kdGYRxXww1xO5nku7sjMjnf7WDKqgfxsbYhBDj3+yGrkPRSbt+5EXuQujuyiP3lfj5PWkaz5mBRaxizH70Z3+929Wn8CUDlg6PQJtbxcPpybrO211DQECdMtdRpdq7eeSdB04u7Na8pGQzISfHk/DqI3BtfO9yexl+qhvFZdiY0GYlaDr7f7mpfsKmXroVDv8wi+JYilqQv7Nh3f+lI1nyQSdSSKo8XezsXlOAg6q5IYfTvN+CrtLH2nkvP2SDSxlxCwX2CXZe/hQGFTW2CZKODJk1Q6PbDKRQe+PyXBO8QBOyoRRSX91p9j8ZbRjL29+t5ImQjAz9/iLS/7OnVgYhssSApSuc2ZYWyh0fgyGxFq/Qi5Y/bPF5htpMMGekcujSA5miJr+5+gQRDuxfgit3Tafgqkohllaj7DvZY+6eUy2ql4vaBZN62k9diVgLQqrkY9vEjJH/c6LEVhw/9IgvrzWX8O3Ue0Yajfqbflo1icU5/gpd64f+fHlhX6yxQ/PywZ6Uy7aVl3Oe/nwnZM/F6IQDD8jMHpvtYvfm64T/4+fnRdIb76qyMj8WLF/Pxxx+zadMmDAYDzz77LBkZGfTv35/Ww1kkK1euZN++ffzlL3/p+Fxra+sZBTlCd42PmnuyiLoln1vCN/DUvJ8S+8wGj89VyVYrko83ktEIigzyYc+HEO2rIB4u3czhuW61ofGiMzygfcol7xex+F5aw6H8AGIXaXivzO7TWgFHkK1WpOhwmtIDqU9WOq70S27cRYRX+8N386FYqhdF41ekYvlic/evE1lBCbAh+VmP7nO62h/mQiBaWnv9tyn6bCBrR75JgOKDS6g8Uj6SA7cntL90W1o9fm+cE5KE7O2NHODfHq91mjiRM6EEBNA6Kpm0J3bxi5DvCVfaeOPQKD7aNpyQ742ErCxBNDQinC6E09mrlT0rZo3is9++wDNlV1Ez1dCrAxLZy4ui32YihjZiXumHLd9FzUAjvpdXEetXx9b1KaS+WXXWa0ydLZLBgGQytf8b6H90MVC7A62pGc3u6PVrsuHWkVSO1rhx2GYeC/3hcLYJ9F/5K1L/bkfkHPDYdSJbLMhWX/Ayd14I9fBzQrTae3w12TMiSUgmE0pIMBgN7TLVN3TJKD0b4+OsfGxXXXVVp7/vvPNOqqurGTp0KD/8cHQVmtbWVioru+ZSNplMmM1H05msVutpzj49SnAQ9WMc3B2Szbulo4hb3NTtBcNOhtbUdM4BcRcTakUVCZ9ZcK72Jai+GbmgrNu1KjyF1tSElOfEWl2LNduvY3/R7jQKD5ezV+wqMXll0Grv9vou7Y0eXrCwDxehOh7HIW+2Oq0MNjUyu3Y4O568BO/927u1TLnHEQKttdUjD121oRGfjQXkPpXBb2yD0RQJc6NKarkdQ2lZ75ZRP46ob6u57dDv8D7kxnxoU6+2rTldhG1qo8TPiu/V1dS6DDgdRuo3h6AdDCF5T3N7UGEPI9zuoy/yPpx+lowm5JR4Dv40iLGTdzLLfw+ZXiVYZRMuofJhUyKxHxggv9ijBqrW0tLnXuEzIgSira3HpwS7NcFns7WvoFlb2/lh+7Of/Yxbb72ViooKvvnmG55++mnsp1jA6LHHHuPJJ5/sjhgdaPERXBJXTK3blwPbYkjZvbNvUtj+jyBcTtTduSi76dUlyLuKaGtrT9c7Zq7SfKBzRkPfr2jRs4R/L/Or1rvR/Nx455uIWbD24r4nNBW1uhrzwmqOr9DQ132t7t2PfxdXK/Y4morX9gKi5QRKtWA0A5hrJSLWtGLMKW43mi9C7+wpERq43JjrJZbt6sdyY9rRQ6qEqcxE0trd582KvxcjZzXt0umDksR///tf/P39GTPmaFXCu+++m8LCQsrKyhg0aBDPPfccGzduZPr06Sf9npN5PkpLS89p2sV55aU0PNBEY5MPkZ8a8f6676v16ejo6Ojo/F+gx6ZdjmXOnDlkZGQwevToTvvfeuutjv/v2rWL8vJyVqxYQWJiInl5J84nOp1OnM4TXcDe1rMv+eyzdjf+xwZKWz2ZOKajo6Ojo6NzKs7mvX1Oxsfs2bOZOnUqY8eOpbT09PNCGzZsACA5OfmkxsfxHIn5+LjkX+cimo6Ojo6Ojk4fYrVaPe/5mD17NjfccAPjxo2joKDgjOcPGTIEgPLy8i59f1lZGWlpaeTm5hIVFdXlLJkLlSPTTLquFxe6rhcnuq4XJ7qunv3+srIzB3aflfExZ84cbrnlFqZNm0ZTUxNhYWEANDQ04HA4SExM5JZbbmHRokUcOnSIQYMG8fLLL7N69Wqys7O73M4RQ6WpqemivxCOoOt6caLrenGi63pxouvqme/tCmdlfNx///0ArF69utP+O++8k/feew+n08mkSZOYNWsWFouF4uJi5s+fzzPPPHM2zejo6Ojo6OhcxJyV8SFJpy9DXVJSckJ1Ux0dHR0dHR2dY/HAesGep62tjSeffJK2PlpSuTfRdb040XW9ONF1vTjRde19zrnOh46Ojo6Ojo7OuXBeej50dHR0dHR0Ll5040NHR0dHR0enV9GNDx0dHR0dHZ1eRTc+dHR0dHR0dHoV3fjQ0dHR0dHR6VXOO+Pj/vvvJz8/H7vdzvr16xk2bFhfi9RtnnjiCYQQnba9e/d2HDebzbz66qvU1NTQ1NTE559/TmhoaB9K3HXGjBnDf//7X0pLSxFCMG3atBPOeeqppygrK6O1tZWlS5eSnJzc6XhAQAAffPABDQ0N1NXV8fbbb2OxWHpLhS5zJl3nzp17Qj8vXry40zkXiq5//OMf2bhxI42NjVRWVvLll1+Smpra6ZyuXLcxMTEsWLCAlpYWKisref7551EUpTdVOSNd0XXlypUn9O3rr7/e6ZwLQdd7772XHTt20NDQQENDA2vXrmXKlCkdxy+WPoUz63qx9OnJePTRRxFC8PLLL3fsOx/7Vpwv24wZM4TD4RB33nmn6Nevn3jzzTdFbW2tCAkJ6XPZurM98cQTIjs7W4SFhXVsQUFBHcdfe+01UVhYKMaPHy8yMzPF2rVrxY8//tjncndlmzJlinj66afF9ddfL4QQYtq0aZ2O/+EPfxB1dXXiuuuuEwMHDhRfffWVOHjwoDCbzR3nLFq0SGzbtk0MHz5cXHbZZWLfvn1i3rx5fa7b2eo6d+5csWjRok797O/v3+mcC0XXxYsXizvuuEP0799fDBo0SCxYsEAUFBQIHx+fLl+3siyLnTt3iu+++04MHjxYTJkyRVRVVYn//d//7XP9zlbXlStXijfffLNT31qt1gtO16lTp4qrrrpKJCcni5SUFPHMM8+ItrY20b9//4uqT7ui68XSp8dvl156qcjLyxPbt28XL7/8csf+87Bv+/7HOrKtX79ezJ49u+NvSZJESUmJePTRR/tctu5sTzzxhNi2bdtJj/n5+Ym2tjYxffr0jn1paWlCCCFGjBjR57KfzXayF3JZWZl45JFHOulrt9vFzJkzBSDS09OFEEIMHTq045wrr7xSqKoqIiIi+lyns9F17ty54ssvvzzlZy5UXQERHBwshBBizJgxHf14put2ypQpwu12i9DQ0I5z7rnnHlFfXy+MRmOf69RVXaH9RXXsg/z47ULVFRCHDh0Sd91110Xdp8frerH2qcViEbm5uWLixImd9Dsf+/a8mXYxGo0MHTqUZcuWdewTQrBs2TKysrL6UDLPkJKSQmlpKQcPHuSDDz4gJiYGgKFDh2IymTrpnZubS2Fh4QWvd0JCAhEREZ10a2xsZMOGDR26ZWVlUVdXx5YtWzrOWbZsGZqmMWLEiF6XubuMGzeOyspKcnJyeO211wgMDOw4diHrarPZAKitrQW6dt1mZWWRnZ1NVVVVxznffvstNpuNAQMG9KL0Z8fxuh7hZz/7GdXV1WRnZ/Pss8/i7e3dcexC1FWWZWbOnInFYmHdunUXdZ8er+sRLrY+nTNnDgsXLmT58uWd9p+PfXtWa7v0JMHBwRgMBiorKzvtr6ysJD09vY+k8gwbNmzgzjvvJDc3l4iICJ544gl++OEHMjIyCA8Pp62tjYaGhk6fqaysJDw8vI8k9gxH5D9Znx45Fh4e3uliB1BVldra2gtO/yVLlvDFF1+Qn59PUlISzz77LIsXLyYrKwtN0y5YXSVJ4h//+Ac//vgju3fvBujSdRseHn7Svj9y7HzkZLoCfPjhhxQWFlJWVsagQYN47rnnSEtLY/r06cCFpWtGRgbr1q3Dy8uL5uZmbrjhBvbu3cuQIUMuuj49la5wcfUpwMyZM8nMzDxpnOT5eL+eN8bHxcySJUs6/p+dnc2GDRsoLCxkxowZ2O32PpRMx5N88sknHf/ftWsXO3fuJC8vj3HjxrFixYo+lKx7zJkzh4yMDEaPHt3XovQ4p9L1rbfe6vj/rl27KC8vZ8WKFSQmJpKXl9fbYnaL3NxchgwZgs1m46abbuK9997j8ssv72uxeoRT6bp3796Lqk+jo6N55ZVXuOKKK/p8zZauct5Mu9TU1OB2uwkLC+u0PywsjIqKij6SqmdoaGhg3759JCcnU1FRgdls7nD1HuFi0PuI/Kfr04qKihMirhVFITAw8ILXPz8/n+rq6o7sngtR19mzZzN16lTGjx9PaWlpx/6uXLcVFRUn7fsjx843TqXrydiwYQNAp769UHR1uVwcPHiQrVu38vjjj7Njxw5+85vfXJR9eipdT8aF3KdDhw4lLCyMrVu34nK5cLlcjBs3joceegiXy0VlZeV517fnjfHhcrnYsmULEydO7NgnSRITJ07sNEd3MWCxWEhKSqK8vJwtW7bgdDo76Z2amkpcXNwFr3d+fj7l5eWddLNarYwYMaJDt3Xr1hEQEEBmZmbHORMmTECW5Y6HwYVKVFQUQUFBlJeXAxeerrNnz+aGG25gwoQJFBQUdDrWlet23bp1DBw4kJCQkI5zrrjiChoaGtizZ0+v6NBVTqfryRgyZAhAp769UHQ9HlmWMZvNF12fnowjup6MC7lPly9fTkZGBkOGDOnYNm3axLx58xgyZAibN28+L/u2zyN0j2wzZswQdrtd3H777SI9PV288cYbora2tlP07YW4vfDCC2Ls2LEiLi5OZGVlie+++05UVVWJ4OBgAe0pUAUFBWLcuHEiMzNTrFmzRqxZs6bP5e7KZrFYxODBg8XgwYOFEELMmjVLDB48WMTExAhoT7Wtra0V1157rcjIyBBffvnlSVNtt2zZIoYNGyZGjRolcnNzz8v009PparFYxPPPPy9GjBgh4uLixIQJE8TmzZtFbm6uMJlMF5yuc+bMEXV1dWLs2LGdUhG9vLw6zjnTdXskdW/JkiVi0KBBYvLkyaKysvK8S1U8k66JiYniT3/6k8jMzBRxcXHi2muvFQcOHBCrVq264HR99tlnxZgxY0RcXJzIyMgQzz77rFBVVUyaNOmi6tMz6Xox9emptuOzec7Dvu37H+nY7YEHHhAFBQXC4XCI9evXi+HDh/e5TN3dPvroI1FaWiocDocoLi4WH330kUhMTOw4bjabxauvvioOHTokmpubxfz580VYWFify92V7fLLLxcnY+7cuR3nPPXUU6K8vFzY7XaxdOlSkZKS0uk7AgICxLx580RjY6Oor68X77zzjrBYLH2u29no6uXlJZYsWSIqKytFW1ubyM/PF2+++eYJhvOFouupuOOOO87quo2NjRULFy4ULS0toqqqSrzwwgtCUZQ+1+9sdI2OjharVq0SNTU1wm63i3379onnnnuuU02IC0XXt99+W+Tn5wuHwyEqKyvF0qVLOwyPi6lPz6TrxdSnp9qONz7Ot76VDv9HR0dHR0dHR6dXOG9iPnR0dHR0dHT+b6AbHzo6Ojo6Ojq9im586Ojo6Ojo6PQquvGho6Ojo6Oj06voxoeOjo6Ojo5Or6IbHzo6Ojo6Ojq9im586Ojo6Ojo6PQquvGho6Ojo6Oj06voxoeOjo6Ojo5Or6IbHzo6Ojo6Ojq9im586Ojo6Ojo6PQq/x8iIKvGKkyx3gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs = np.random.randint(0, len(train_imgs), 15)\n",
    "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
    "plt.imshow(imgs)\n",
    "print(\"Labels:\", train_labels[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFP9QnUJuJuY"
   },
   "source": [
    "### Standardizarea datelor\n",
    "\n",
    "Datele de intrare (imaginile) vor fi rescalate pentru a avea media zero și deviația standard 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWmt4XVxuJuZ"
   },
   "outputs": [],
   "source": [
    "mean, std  = train_imgs.mean(), train_imgs.std()\n",
    "train_imgs = (train_imgs - mean) / std\n",
    "test_imgs = (test_imgs - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justificare:\n",
    "\n",
    "- __Media__ poate fi considerata diferenta dintre o imagine mai luminoasa si una mai intunecoasa, fapt ce nu ar trebui sa influenteze predictia retelei. Avand media zero, rețeaua va fi indiferentă la variatii in luminozitatea imaginilor. In caz contrar ar trebui sa isi modifice termenii de bias pentru a compensa diferentele.\n",
    "\n",
    "- __Varianta__ poate fi considerata diferenta dintre o imagine cu mai mult contrast si una cu mai putin contrast, din nou, diferente ce nu ar trebui sa afecteze predictia retelei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgERUA07IuSr"
   },
   "source": [
    "## 2. Construirea unei rețele de tip feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE-V6sONuJud"
   },
   "source": [
    "### Notații\n",
    "  - dimensiunea datelor de intrare este $D = 28 * 28 = 784$, iar dimensiunea ieșirilor rețelei este $K=10$ (numărul de clase)\n",
    "  - rețeaua neurală va avea $L$ straturi\n",
    "  - $B$ va reprezenta dimensiunea batch-ului (numărul de exemple trecute în același timp prin rețea)\n",
    "  - Vom nota cu ${\\bf X} \\in {\\mathbb R}^{B \\times D}$ un batch de intrări $\\left\\lbrace {\\bf x}_0, {\\bf x}_1, \\dots {\\bf x}_B \\right\\rbrace$ și similar ${\\bf Y} \\in {\\mathbb R}^{B \\times K}$\n",
    "  - ${\\bf x}^{(l)}$ reprezintă intrările stratului $l$ (${\\bf x}^{(0)}$ va fi o imagine precum cele din setul MNIST de dimensiune $D$)\n",
    "  - ${\\bf y}^{(l)}$ reprezintă ieșirile stratului $l$ (${\\bf y}^{(L-1)}$ reprezintă ieșirile rețelei)\n",
    "  - ${\\bf \\theta}^{(l)}$ reprezintă parametrii stratului $l$\n",
    "  - ${\\cal L}$ reprezintă funcția de cost ( _negative log likelihood_ )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YTu4tS8uJue"
   },
   "source": [
    "### Straturile rețelei\n",
    "\n",
    "Unele straturi au parametri ce trebuie optimizați în timpul antrenării. Vom nota parametrii stratului $l$ cu $\\bf{\\theta}^{(l)}$.\n",
    "Fiecare strat pe care îl veți implementa va avea trei metode:\n",
    " - `forward` calculează și întoarce ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf \\theta}^{(l)}\\right)$\n",
    " - `backward` primește $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, reține intern $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}^{(l)}}$ și întoarce $\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l)}}$\n",
    " - `update` modifică parametrii locali ${\\bf \\theta}^{(l)}$ folosing gradientul stocat $\\frac{\\partial{\\cal L}}{\\partial{\\bf \\theta}^{(l)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW206j3euJuf"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass  # If a layer has no parameters, then this function does nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIT6K4IduJuk"
   },
   "source": [
    "### Rețeaua neurală\n",
    "\n",
    "  * în faza `forward` ieșirile stratului $l$ devin intrările stratului $l+1$: ${\\bf x}^{(l+1)} = {\\bf y}^{(l)}$\n",
    "  * în faza `backward` gradientul în raport cu intrările stratului $l+1$ devine gradientul în raport cu ieșirile stratului $l$: $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}=\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l+1)}}$\n",
    "  \n",
    "**[Cerința 0]** Completați metoda `backward` din clasa `FeedForwardNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTn-g3KAuJul"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
    "        self._inputs = []\n",
    "        for layer in self.layers:\n",
    "            if train:\n",
    "                self._inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <0> : Calculati gradientul cu fiecare strat\n",
    "        # Pasi:\n",
    "        #   - iterati in ordine inversa prin straturile retelei si apelati pentru fiecare dintre ele metoda backward\n",
    "        #   - folositi self._inputs salvate la fiecare pas din forward pentru a calcula gradientul cu respectivul strat\n",
    "        #   - transmiteti mai departe valoarea returnata de metoda backward catre urmatorul strat\n",
    "        #   - incepeti cu gradientul fata de output (dy, primit ca argument).\n",
    "        for x, layer in zip(reversed(self._inputs), reversed(self.layers)):\n",
    "            dy = layer.backward(x, dy)\n",
    "        return dy\n",
    "    \n",
    "        del self._inputs\n",
    "    \n",
    "    def update(self, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            layer.update(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyQLVM4quJup"
   },
   "source": [
    "### Stratul linear\n",
    "\n",
    "Un strat linear cu $M$ intrări și $N$ ieșiri are parametrii $\\theta = \\left( {\\bf W}, {\\bf b} \\right)$ unde ${\\bf W} \\in \\mathbb{R}^{M \\times N}$ și ${\\bf b} \\in \\mathbb{R}^{N}$.\n",
    "\n",
    "Pentru un singur exemlu ${\\bf x} \\in {\\mathbb R}^{M}$:\n",
    "$$ {\\bf y} = {\\bf x}^{\\intercal}{\\bf W} + {\\bf b} $$\n",
    "\n",
    "**[Cerința 1]** Implementați metoda `forward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și întoarce ieșirile corespunzătoare: $Y \\in {\\mathbb R}^{B\\times N}$.\n",
    "\n",
    "**[Cerința 2]** Implementați metoda `backward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și gradientul în raport cu ieșirile $\\frac{\\partial {\\cal L}}{\\partial {\\bf Y}}$ și realizează două lucruri:\n",
    "  - calculează și salvează intern gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}}$\n",
    "    - `self.dweight =` $ X^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} $\n",
    "    \n",
    "    - `self.dbias =` $ \\sum_{i=1}^{batch size} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}_i} $\n",
    "\n",
    "  - calculează și întoarce gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf X}}$\n",
    "    \n",
    "    - `return ` $ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\theta^T $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S47ZsyKdE7FF"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, insize: int, outsize: int) -> None:\n",
    "        bound = np.sqrt(6. / insize)\n",
    "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
    "        self.bias = np.zeros((outsize,))\n",
    "        \n",
    "        self.dweight = np.zeros_like(self.weight)\n",
    "        self.dbias = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <1>: calculați ieșirea unui strat liniar\n",
    "        # x - este o matrice numpy B x M, unde \n",
    "        #    B - dimensiunea batchului, \n",
    "        #    M - dimensiunea caracteristicilor de intrare (insize)\n",
    "        # Sugestie: folosiți înmulțirea matricială numpy pentru a implementa propagarea înainte într-o singură trecere\n",
    "        # pentru toate exemplele din batch\n",
    "        self.x = x\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <2> : calculați dweight, dbias și returnați dx\n",
    "        # x - este o matrice numpy B x M, unde \n",
    "        #     B - dimensiunea batchului, \n",
    "        #     M - dimensiunea caracteristicilor (features) de intrare (insize)\n",
    "        # dy - este o matrice numpy B x N, unde \n",
    "        #     B - dimensiunea batchului, \n",
    "        #     N - dimensiunea caracteristicilor (features) de ieșire (outsize)\n",
    "        # Sugestie: folosiți înmulțirea matricială numpy pentru a implementa propagarea înapoi într-o singură trecere \n",
    "        #       pentru self.dweight\n",
    "        # Sugestie: folosiți numpy.sum pentru a implementa propagarea înapoi într-o singură trecere pentru self.dbias\n",
    "\n",
    "        self.dweight = np.dot(self.x.T, dy)\n",
    "        self.dbias = np.sum(dy, axis=0)\n",
    "        return np.dot(dy, self.weight.T)\n",
    "    \n",
    "    def update(self, mode='SGD', lr=0.001, mu=0.9):\n",
    "        if mode == 'SGD':\n",
    "            self.weight -= lr * self.dweight\n",
    "            self.bias -= lr * self.dbias\n",
    "        else:\n",
    "            raise ValueError('mode should be SGD, not ' + str(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgfHlVgDuJut"
   },
   "source": [
    "### Functia Rectified Linear Unit\n",
    "\n",
    "Stratul ReLU aplică următoare următoare transformare neliniară element cu element  (vezi grafic):\n",
    "$$y = \\max\\left(x, 0\\right)$$\n",
    "\n",
    "Prin urmare gradientul (derivata) functiei ReLU este:\n",
    "\n",
    "$$ \\text{ReLU}'(x) = \\begin{cases} \n",
    "0 & \\text{pentru } x < 0 \\\\\n",
    "1 & \\text{pentru } x > 0 \\\\\n",
    "\\text{nedefinit} & \\text{pt } x = 0 \n",
    "\\end{cases} $$\n",
    "\n",
    "Pentru a evita portiunea nedefinita, conventia in ML este sa consideram ca gradientul in 0 este 0 (uneori considerat si ca 0.5). Astfel, gradientul devine:\n",
    "\n",
    "$$ \\text{ReLU}'(x) = \\begin{cases} \n",
    "0 & \\text{pentru } x \\leq 0 \\\\\n",
    "1 & \\text{pentru } x > 0 \n",
    "\\end{cases} $$\n",
    "\n",
    "**[Cerințele 3-4]** Implementați metodele `forward` și `backward` pentru un strat de activare ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOR1DJiwE7FJ"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <3> : Calculați ieșirea unei unități liniare rectificate\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <4> : Calculați gradientul față de x\n",
    "        # x - este o matrice numpy B x M, unde B - dimensiunea batchului, M - dimensiunea caracteristicilor\n",
    "        # Sugestie: utilizați indexarea logică numpy pentru a determina unde intrarea (x) este negativă\n",
    "        #       și faceți gradientul 0 pentru acele exemple\n",
    "        return dy * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NrWBTmbI9gW"
   },
   "source": [
    "## 3. Funcția de cost\n",
    "\n",
    "Funcția de cost pe care o vom folosi este _cross entropy_ care combină un _softmax_ și un cost _negative log-likelihood_. (Matematica la tablă)\n",
    "\n",
    "Dacă ${\\bf y}$ reprezintă ieșrile rețelei pentru o intrare ${\\bf x}$, atunci ${\\bf y}$ va avea o dimensiune egală cu numărul de clase $K$. Atunci probabilitatea (prezisă de rețea) ca exemplul ${\\bf x}$ să aparțină clasei $k$ va fi $p_k$:\n",
    "$$\\begin{align}\n",
    "p_k &= \\frac{e^{y_k}}{\\sum_j e^{y_j}} & & \\text{softmax} \\\\\n",
    "{\\cal L} &= -\\log p_t & & \\text{negative log-likelihood}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Pentru un batch de dimensiune $B$ se va face media costurilor corespunzătare fiecărui exemplu ($p_k$ este o funcție de ${\\bf x}$ și ${\\bf \\theta}$):\n",
    "\n",
    "$$ {\\cal L} = \\frac{1}{B} \\sum_{({\\bf x}, {\\bf t}) \\in Batch} -\\log p_t \\left({\\bf x}, \\theta\\right) $$\n",
    "\n",
    "Derivata softmax-ului în raport cu logits se calculează astfel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_k}{\\partial y_i} = p_k (\\delta_{ki} - p_i)\n",
    "$$\n",
    "\n",
    "unde $\\delta_{ki}$ este delta Kronecker, care este 1 dacă $k = i$ și 0 altfel:\n",
    "\n",
    "$$\n",
    "\\delta_{ij} = \n",
    "\\begin{cases} \n",
    "1 & \\text{pentru } i = j \\\\\n",
    "0 & \\text{pentru } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Derivata funcției de cost în raport cu logit-ul pentru clasa $i$ devine:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial {\\cal L}}{\\partial y_i} = p_i - \\delta_{ti}\n",
    "$$\n",
    "\n",
    "pentru orice clasă $i$, unde $t$ este clasa adevărată.\n",
    "\n",
    "În contextul unui batch de exemple, formula pentru calculul gradientului mediu în raport cu fiecare parametru $\\theta$ al rețelei este:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial {\\cal L}}{\\partial \\theta} = \\frac{1}{B} \\sum_{({\\bf x}, {\\bf t}) \\in Batch} \\sum_{i} \\left(p_i - \\delta_{ti}\\right) \\frac{\\partial y_i}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Unde $\\frac{\\partial y_i}{\\partial \\theta}$ este gradientul logit-ului $y_i$ în raport cu parametrii $\\theta$.\n",
    "\n",
    "\n",
    "**[Cerințele 5-6]** Implementați metodele `forward` și `backward` pentru un funcția de cost _cross-entropy_ (o vom privi ca pe un strat suplimentar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDXiDEu8E7FW"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps,axis = 1).reshape(-1,1)\n",
    "\n",
    "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
    "        # TODO <5> : Calculați probabilitatea logaritmică negativă\n",
    "        # y - matrice numpy (B, K), unde B - dimensiunea batch-ului, K - numărul de clase (numărul de logaritmi)\n",
    "        # t - vector numpy (B, ), unde B - dimensiunea batch-ului, care indică clasa corectă\n",
    "        # Pasi: \n",
    "        #   - folositi softmax() pe intrari pentru a transforma logits (y) in probabilitati\n",
    "        #   - selectati probabilitatile care corespund clasei reale (t)\n",
    "        #   - calculati -log() peste probabilitati\n",
    "        #   - impartiti la batch size pentru a calcula valoarea medie peste toate exemplele din batch\n",
    "        self.y = y\n",
    "        self.t = t\n",
    "        self.p = self.softmax(y)\n",
    "        return -np.mean(np.log(self.p[np.arange(len(t)), t]))\n",
    "    \n",
    "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        # TODO <6> : Calculati dl/dy\n",
    "        # Pasi: \n",
    "        #   - calculati softmax(y) pentru a determina probabilitatea ca fiecare element sa apartina clasei i\n",
    "        #   - ajustati gradientii pentru clasa corecta: aplicati scaderea dL/dy_i = pi - delta_ti conform formulelor de mai sus\n",
    "        #   - impartiti la batch size pentru a calcula valoarea medie peste toate exemplele din batch\n",
    "        self.p = self.softmax(y)\n",
    "        self.p[np.arange(len(t)), t] -= 1\n",
    "        return self.p / len(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uz9qM5eHJLNw"
   },
   "source": [
    "### Acuratețea\n",
    "\n",
    "**[Cerința 7]** Calculați acuratețea predicțiilor ${\\bf y}$ în raport cu clasele corecte ${\\bf t}$ (rația exemplelor pentru care clasa corectă a avut probabilitatea prezisă maximă)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nYfVCBSE7Fe"
   },
   "outputs": [],
   "source": [
    "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
    "    # TODO <7> : Calculati acuratetea\n",
    "    # Pasi: \n",
    "    # - folosiți np.argmax() pentru a afla predictiile retelei\n",
    "    # - folositi np.sum() pentru a numara cate sunt corecte comparand cu ground truth (t)\n",
    "    # - impartiti la batch size pentru a calcula valoarea medie peste toate exemplele din batch\n",
    "    return np.mean(np.argmax(y, axis=1) == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7zGgHlduJvA"
   },
   "source": [
    "## Teste\n",
    "\n",
    "Executați ```test0() and test16() and test7()``` pentru a rula testele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YaLsPBfuJvB"
   },
   "outputs": [],
   "source": [
    "def test0():\n",
    "    fakex = [np.random.randn(128, n) for n in [20, 40, 30, 10]]\n",
    "\n",
    "    class DummyLayer:\n",
    "        def __init__(self, idx):\n",
    "            self.idx = idx\n",
    "\n",
    "        def forward(self, x):\n",
    "            return fakex[self.idx + 1]\n",
    "\n",
    "        def backward(self, x, dldy):\n",
    "            if not np.allclose(x, fakex[self.idx]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            if not np.allclose(dldy, -fakex[self.idx+1]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            return -x\n",
    "\n",
    "    try:\n",
    "        net = FeedForwardNetwork([DummyLayer(i) for i in range(3)])\n",
    "        net.forward(fakex[0])\n",
    "        net.backward(-fakex[-1])\n",
    "        print(\"Cerința 0 rezolvată corect!\")\n",
    "        return True\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 0 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 0 are erori.\")\n",
    "        \n",
    "    return False\n",
    "        \n",
    "def test16():\n",
    "    __x = np.array([[-3.0731, -1.9081, -0.7283, -0.0757, -0.7577],\n",
    "                    [ 2.4041, -1.1506, -0.5924,  1.3016,  1.0882],\n",
    "                    [-0.5254,  0.3519, -0.9633, -2.7393, -0.9745]])\n",
    "    __w = np.array([[ 1.3214, -0.5886, -0.0351,  1.2084,  1.2661, -0.9979, -0.1172],\n",
    "                    [-0.4022,  0.1168,  0.9020, -2.0098, -0.5409, -0.3876, -0.1719],\n",
    "                    [-1.1125, -0.5556,  0.8843,  0.6995,  0.4929,  0.7523,  0.1832],\n",
    "                    [ 0.2267,  0.6757,  1.1286, -0.3218,  1.6934, -0.1782, -0.3467],\n",
    "                    [-0.6062,  0.4426,  0.5090,  0.4772, -0.5721,  0.8658, -0.5999]])\n",
    "    __b = np.array([ 0.3335,  0.5051, -0.1393,  1.2116,  1.7836, -0.6597,  0.3553])\n",
    "    __y = np.array([[-1.70746622, 2.10919555, -2.8676804, 0.48630531, -1.1288499, 1.95609904, 1.39083457],\n",
    "                    [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, -2.34822291, -0.94127596],\n",
    "                    [0.5391161, -0.89159687, -4.24288533, -0.38789499, -3.62798139, -1.35206921, 1.71422657]])\n",
    "    \n",
    "    __dy = np.array([[ 1.5555, -0.8978, -0.2917, -0.3868, -0.8257, -0.3491, -0.8658],\n",
    "                     [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0.4794,  0.8565],\n",
    "                     [-0.1552, -1.6319,  1.7642,  1.0503,  0.1035 , -0.7186, -0.9782]])\n",
    "    __dx = np.array([[ 1.53113221,  0.51455541, -2.588423,   -1.49460989, -0.98384103],\n",
    "                     [ 0.41215308,  0.46469672, -0.59552791,  3.04147235, -0.08763244],\n",
    "                     [ 2.92549149, -0.25707023,  2.70531668,  1.15769427,  0.67643021]])\n",
    "    __dw = np.array(\n",
    "        [[-2.01905511,  7.20190418,  2.2752849,   0.00865613,  3.89837344,  2.60289719, 5.23374791],\n",
    "         [-4.30512319, -0.57717827,  0.07387429,  1.40830543,  0.9345816,  -0.13835527, 0.3223155 ],\n",
    "         [-1.64365553,  1.34237165, -2.05517959, -0.57525343,  0.15290988,  0.66248035, 1.0654716 ],\n",
    "         [ 1.75815137,  6.47943337, -3.56222681, -3.18791411,  0.54523986,  2.61887489, 3.85994472],\n",
    "         [ 0.18554777,  3.89349109, -0.45449919, -1.01478565,  1.16539548,  1.48647185, 2.54131586]]\n",
    "    )\n",
    "    __db = np.array([ 2.5149, -1.0383,  2.4316,  0.4022, -0.1335, -0.5883, -0.9875])\n",
    "    \n",
    "    __y_relu = np.array([[0, 2.10919555, 0, 0.48630531, 0, 1.95609904, 1.39083457],\n",
    "                         [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, 0, 0],\n",
    "                         [0.5391161, 0, 0, 0, 0, 0, 1.71422657]])\n",
    "    __drelu = np.array([[0, -0.8978, 0, -0.3868, 0, -0.3491, -0.8658],\n",
    "                        [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0,  0],\n",
    "                        [-0.1552, 0,  0,  0,  0 , 0, -0.9782]])\n",
    "    \n",
    "    __t = np.array([3, 1, 2])\n",
    "    __dl_dy = np.array(\n",
    "        [[ 2.80870645e-03,  1.27661957e-01,  8.80302096e-04, -3.08142112e-01,\n",
    "           5.00952130e-03,  1.09539948e-01,  6.22416775e-02],\n",
    "         [ 1.73238217e-02, -3.32870086e-01,  3.07917841e-04,  1.09927743e-01,\n",
    "           2.05192672e-01,  2.31991342e-05,  9.47329526e-05],\n",
    "         [ 6.60308812e-02,  1.57905168e-02, -3.32780047e-01,  2.61307149e-02,\n",
    "           1.02329216e-03,  9.96358772e-03,  2.13841054e-01]]\n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "        lin = Linear(5, 7)\n",
    "        lin.weight = __w.copy()\n",
    "        lin.bias = __b.copy()\n",
    "        y = lin.forward(__x.copy())\n",
    "        if not np.allclose(y, __y):\n",
    "            raise Exception(\"Ieșiri greșite\")\n",
    "        print(\"Cerința 1 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 1 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 1 are erori.\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        dx = lin.backward(__x.copy(), __dy.copy())\n",
    "        if not np.allclose(dx, __dx):\n",
    "            raise ValueError(\"dL/dx greșit\")\n",
    "        if not np.allclose(lin.dweight, __dw):\n",
    "            raise ValueError(\"dL/dw greșit\")\n",
    "        if not np.allclose(lin.dbias, __db):\n",
    "            raise ValueError(\"dL/db greșit\")\n",
    "        print(\"Cerința 2 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 2 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 2 are erori.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        y_relu = relu.forward(__y.copy())\n",
    "        if not np.allclose(y_relu, __y_relu):\n",
    "            raise ValueError(\"ReLU(x) greșit\")\n",
    "        print(\"Cerința 3 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 3 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 3 are erori.\")\n",
    "        return False\n",
    "            \n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        drelu = relu.backward(__y.copy(), __dy.copy())\n",
    "        if not np.allclose(drelu, __drelu):\n",
    "            raise ValueError(\"ReLU.backward greșit\")\n",
    "        print(\"Cerința 4 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 4 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 4 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        loss = ce.forward(__y.copy(), __t.copy())\n",
    "        if np.abs(loss - 5.1874357237332545) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită nll: {loss:f} în loc de 5.1874357237332545\")\n",
    "        print(\"Cerința 5 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 5 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 5 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        dl_dy = ce.backward(__y.copy(), __t.copy())\n",
    "        if not np.allclose(dl_dy, __dl_dy) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită pentru dNLL/dy\")\n",
    "        print(\"Cerința 6 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 6 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 6 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test7():  # Acuratețea\n",
    "    y = np.array([[ 0.6460014 , -0.05876393, -1.36496105, -0.07057596,  0.54938383],\n",
    "                  [-0.8033942 , -0.51753041,  0.92278036, -1.66303585, -0.36537512],\n",
    "                  [-1.3710599 ,  0.65598193, -0.75527154,  1.21609284,  0.08284123],\n",
    "                  [-1.24696857,  0.32676634,  0.09572539,  1.38316398, -0.14110726],\n",
    "                  [-2.01698315,  2.06123375, -1.68003675,  0.0504592 ,  0.04427597],\n",
    "                  [-0.8893451 ,  1.74695148, -0.29394473,  0.74203068, -0.75185261],\n",
    "                  [ 1.34126333, -0.5272606 ,  1.46458319,  1.59529987,  1.86884676],\n",
    "                  [-0.58987297,  1.10900165, -0.71208103,  0.20478154, -1.26693567],\n",
    "                  [-2.17730677, -1.36147532, -1.49679182,  0.24812177, -0.13368035],\n",
    "                  [-0.48730599,  1.31710647,  0.41765538,  1.19869192, -0.05301611],\n",
    "                  [-0.10655224, -0.21174034,  1.31548647, -0.57990281,  0.85868472],\n",
    "                  [-0.32055613, -2.17817118, -0.28488692,  1.62977524,  0.25150929],\n",
    "                  [ 0.07704727,  1.67710047,  1.83368441, -0.45456845, -0.74474969]])\n",
    "    t = np.array([0, 2, 3, 3, 1, 0, 1, 1, 2, 1, 2, 3, 2])\n",
    "    try:\n",
    "        acc = accuracy(y, t)\n",
    "        if np.abs(acc - 0.7692307692307693) > 1e-7:\n",
    "            raise ValueError(f\"{acc:f} != 10/13\")\n",
    "        print(f\"Cerința 7 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 7 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 7 are erori.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWQ2e7C4uJvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerința 0 rezolvată corect!\n",
      "Cerința 1 rezolvată corect!\n",
      "Cerința 2 rezolvată corect!\n",
      "Cerința 3 rezolvată corect!\n",
      "Cerința 4 rezolvată corect!\n",
      "Cerința 5 rezolvată corect!\n",
      "Cerința 6 rezolvată corect!\n",
      "Cerința 7 rezolvată corect!\n"
     ]
    }
   ],
   "source": [
    "test0() and test16() and test7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIhtzd2gJQF2"
   },
   "source": [
    "## 4. Antrenarea rețelei neurale\n",
    "\n",
    "**[Cerința 8]** Completați codul de mai jos pentru a calcula gradientul funcției de cost pentru batchul ales și parametrii curenți ai rețelei.\n",
    "\n",
    "_Indiciu_: trebuie să apelați metodele `forward` și `backward` ale rețelei neurale și ale funcției de cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTbmZv3YE7Fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Batch 030 | Train NLL:  1.359 | Train Acc:  60.94% "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Batch 468 | Train NLL:  0.482 | Train Acc:  91.67% | Test NLL:  0.414 | Test Acc: 88.38%\n",
      "Epoch 02 | Batch 468 | Train NLL:  0.367 | Train Acc:  93.75% | Test NLL:  0.328 | Test Acc: 90.57%\n",
      "Epoch 03 | Batch 468 | Train NLL:  0.327 | Train Acc:  94.79% | Test NLL:  0.289 | Test Acc: 91.49%\n",
      "Epoch 04 | Batch 468 | Train NLL:  0.304 | Train Acc:  95.83% | Test NLL:  0.264 | Test Acc: 92.28%\n",
      "Epoch 05 | Batch 468 | Train NLL:  0.290 | Train Acc:  96.88% | Test NLL:  0.246 | Test Acc: 92.85%\n",
      "Epoch 06 | Batch 468 | Train NLL:  0.280 | Train Acc:  96.88% | Test NLL:  0.232 | Test Acc: 93.43%\n",
      "Epoch 07 | Batch 468 | Train NLL:  0.271 | Train Acc:  96.88% | Test NLL:  0.220 | Test Acc: 93.72%\n",
      "Epoch 08 | Batch 468 | Train NLL:  0.264 | Train Acc:  96.88% | Test NLL:  0.210 | Test Acc: 94.02%\n",
      "Epoch 09 | Batch 468 | Train NLL:  0.258 | Train Acc:  96.88% | Test NLL:  0.201 | Test Acc: 94.27%\n",
      "Epoch 10 | Batch 468 | Train NLL:  0.252 | Train Acc:  96.88% | Test NLL:  0.193 | Test Acc: 94.35%\n",
      "Epoch 11 | Batch 468 | Train NLL:  0.247 | Train Acc:  96.88% | Test NLL:  0.187 | Test Acc: 94.53%\n",
      "Epoch 12 | Batch 185 | Train NLL:  0.217 | Train Acc:  93.75% "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m t \u001b[38;5;241m=\u001b[39m train_labels[idx:idx \u001b[38;5;241m+\u001b[39m BATCH_SIZE]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 2. Calculam gradientul\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TODO <8>:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Hint: propagam batch-ul `x` prin reteaua `net`\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#       calculam eroarea pe baza iesirii retelei, folosind `cost_function` \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#       obtinem gradientul erorii in raport cu iesirea retelei, folosind `backward` pentru `cost_function`\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#       obtinem gradientul in raport cu ponderile retelei `net` folosind `backward` pentru `net`\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m cost_function\u001b[38;5;241m.\u001b[39mforward(y, t)\n\u001b[1;32m     26\u001b[0m dy \u001b[38;5;241m=\u001b[39m cost_function\u001b[38;5;241m.\u001b[39mbackward(y, t)\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mFeedForwardNetwork.forward\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# TODO <1>: calculați ieșirea unui strat liniar\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# x - este o matrice numpy B x M, unde \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Sugestie: folosiți înmulțirea matricială numpy pentru a implementa propagarea înainte într-o singură trecere\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# pentru toate exemplele din batch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "HIDDEN_UNITS = 300\n",
    "EPOCHS_NO = 20\n",
    "\n",
    "optimize_args = {'mode': 'SGD', 'lr': .005}\n",
    "\n",
    "net = FeedForwardNetwork([Linear(784, HIDDEN_UNITS),\n",
    "                          ReLU(),\n",
    "                          Linear(HIDDEN_UNITS, 10)])\n",
    "cost_function = CrossEntropy()\n",
    "\n",
    "for epoch in range(EPOCHS_NO):\n",
    "    for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
    "        # 1. Pregatim urmatorul batch\n",
    "        x = train_imgs[idx:idx + BATCH_SIZE,:,:].reshape(-1, 784)\n",
    "        t = train_labels[idx:idx + BATCH_SIZE]\n",
    "        \n",
    "        # 2. Calculam gradientul\n",
    "        # TODO <8>:\n",
    "        # Hint: propagam batch-ul `x` prin reteaua `net`\n",
    "        #       calculam eroarea pe baza iesirii retelei, folosind `cost_function` \n",
    "        #       obtinem gradientul erorii in raport cu iesirea retelei, folosind `backward` pentru `cost_function`\n",
    "        #       obtinem gradientul in raport cu ponderile retelei `net` folosind `backward` pentru `net`\n",
    "        y = net.forward(x)\n",
    "        loss = cost_function.forward(y, t)\n",
    "        dy = cost_function.backward(y, t)\n",
    "        net.backward(dy)\n",
    "        \n",
    "        # 3. Actualizam parametrii retelei\n",
    "        net.update(**optimize_args)\n",
    "        \n",
    "        print(f'\\rEpoch {epoch + 1:02d} '\n",
    "              f'| Batch {b_no:03d} '\n",
    "              f'| Train NLL: {loss:6.3f} '\n",
    "              f'| Train Acc: {accuracy(y, t) * 100:6.2f}% ', end='')\n",
    "\n",
    "    y = net.forward(test_imgs.reshape(-1, 784), train=False)\n",
    "    test_nll = cost_function.forward(y, test_labels)\n",
    "    print(f'| Test NLL: {test_nll:6.3f} '\n",
    "          f'| Test Acc: {accuracy(y, test_labels) * 100:3.2f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Laborator 8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
